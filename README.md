
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
   

</head>
<body>
  



<h1>ğŸ’ª Empowering Mistral7B Instruct with LoRa: ğŸ¤– Specializing in DoD's Fiscal Insights ğŸ’° </h1>

<h2>Introduction</h2><p>
    Fine-tuning Large Language Models (LLMs) ğŸ§  unlocks their true potential âœ¨, enabling them to excel ğŸ† at specialized tasks and address domain-specific knowledge gaps ğŸ•³ï¸. In our project ğŸš€, we employed the LoRa method ğŸ› ï¸ to fine-tune our LLM, Mistral7B Instruct ğŸŒ¬ï¸. This process empowered ğŸ’ª Mistral7B Instruct with the ability to answer intricate questions â“ about the Department of Defense's (DoD) budget ğŸ’° for fiscal year 2024 ğŸ“…, a domain where it previously lacked expertise ğŸš«ğŸ¤·.
</p>

<h2>ğŸ—‚ Repository Structure</h2>
<p>
  ğŸ“ This repository consists of three key files: two Python scripts and one Jupyter notebook.
</p>

<ul>
  <li>ğŸ—ƒ Database Creation Script:
    <a href="https://github.com/GhaithMag/Fine_tune_LLM_LoRa_Mistral7B_Instruct/blob/main/prepare_dataset_LLM.py">prepare_dataset_LLM.py</a> 
  </li>
  <li>ğŸš€ Model Training Script:
    <a href="https://github.com/GhaithMag/Fine_tune_LLM_LoRa_Mistral7B_Instruct/blob/main/train_llm_lora.py">train_llm_lora.py</a> 
  </li>
  <li>ğŸ“˜ Model Training Guide:
    <a href="https://github.com/GhaithMag/Fine_tune_LLM_LoRa_Mistral7B_Instruct/blob/main/mistral-finetune-own-data.ipynb">mistral-finetune-own-data.ipynb</a> 
  </li>
</ul>



<h2>ğŸ“‹ Prerequisites</h2>

<p>ğŸ§ Ensure you are using a Linux environment because the <code>bitsandbytes</code> library, which is essential for our task, only works on Linux as of now.</p>

<p>ğŸ“¥ The dataset required for this project can be downloaded from <a href="https://www.congress.gov/bill/118th-congress/house-bill/4365/text?format=txt">here</a>.</p>

<p>ğŸ”‘ Access to a LLM API is necessary for creating your own dataset using the <code>prepare_dataset_LLM.py</code> script. For this project, we used LM Studio. Feel free to use any API you want. ğŸ˜Š</p>

<h2>ğŸš€ Getting Started</h2>

<p>ğŸ‘©â€ğŸ’» Once you have cloned the repository, start by running the <code>create-data.py</code> script to generate your dataset. Then, proceed to explore the provided <a href="https://github.com/GhaithMag/Fine_tune_LLM_LoRa_Mistral7B_Instruct/blob/main/mistral-finetune-own-data.ipynb">notebook</a> to understand the training process in detail.</p>

<p>ğŸ’¡ Fine-tuning an LLM has never been easier. This project provides you with the necessary tools and guidance to customize an LLM to your specific needs.</p>

<img src="fine_tune_ill.jpg" alt="Illustration of LLM Fine-Tuning" style="width:100%; max-width:200px; display:block">

