{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "\n",
    "<center>\n",
    "<h1 style=\"font-size: 36px;\">Fine-Tuning a Large Language Model with Custom Data: A Comprehensive Guide</h1>\n",
    "</center>\n",
    "\n",
    "\n",
    "<p>Hello everyone!</p>\n",
    "<p>In this notebook, we will learn how to fine-tune a Large Language Model (LLM) with our own dataset using the LoRa method.</p>\n",
    "\n",
    "### 0) Prerequisites\n",
    "\n",
    "<p>Ensure you are using a Linux environment because the <code>bitsandbytes</code> library, which is very important for our task, only works on Linux as of now.<p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U datasets scipy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Create your own data_set\n",
    "<body>\n",
    "    <p>Before diving into this notebook, ensure you've created your dataset using the <code>prepare_dataset.py</code> script (you'll need to tweak the API endpoint to call your LLM, for which I utilized LM Studio. Download your text from <a href=\"https://www.congress.gov/bill/118th-congress/house-bill/4365/text?format=txt\">https://www.congress.gov/bill/118th-congress/house-bill/4365/text?format=txt</a>).</p>\n",
    "    <p>This script initially dispatches a segment of the text to the LLM to concoct questions. Subsequently, an agent scrutinizes the format's accuracy (aiming for a Python list). If it deviates, a correction agent steps in to formulate a Python list.</p>\n",
    "    <p>Following this, we propel these questions towards the LLM to elicit responses. Post acquiring our inputs and outputs, we proceed to craft a .jsonl file embodying our dataset.</p>\n",
    "    <p>Your dataset would mirror the structure: <code>{'input': 'Enlighten me about the legislation...', 'output': 'The legislation encompasses...'}</code></p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_dataset_LLM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='validation.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### II) Loading Mistral-7B-Instruct-v0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    " Here, we are lading our LLM, right? We're using this cool thing called BitsAndBytes because it lets us squish down the model's size without losing the good stuff. That means we can run it even if we don't have a monster computer. We're picking a specific model from mistralai, and the BitsAndBytes magic makes it use less memory by turning stuff into 4-bit instead of the usual bigger size. Pretty neat for keeping things speedy and light!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### III) Tokenization\n",
    "\n",
    "  <p>Alright, so here's the scoop: we're setting up this thing called a tokenizer, which is like the brain's way of understanding and breaking down the stuff we feed it. We're doing this nifty trick where we pad stuff on the left side, and believe it or not, this actually helps the whole setup chug along using less memory. There's this <a href=\"https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\">link</a> that dives into why it's cool.</p>\n",
    "\n",
    "  <p>Now, about the <code>model_max_length</code>, it's kinda like knowing your limits. We first let our tokenizer run wild without holding it back with truncation or padding to see how long the stuff we're dealing with usually is.</p>\n",
    "\n",
    " <p>We've formatted our data in the Mistral instruction format using the <code>prompt_mistral</code> function.</p>\n",
    "\n",
    "\n",
    "  <p>Then, we're bringing in our tokenizer from the pretrained model, telling it to pad on the left, and making sure it knows when a sentence starts and ends. We're also setting the pad token to be the same as the end-of-sentence token, which is a bit like using a period to say \"we're done here\" and fill up space at the same time.</p>\n",
    "\n",
    "  <p> We've got this function <code>generate_and_tokenize_prompt</code> that takes our prompt, runs it through our special formatting function, and tells the tokenizer to keep it under 400 tokens, filling in the gaps with padding if it's too short. Then, it makes a copy of the tokenized input as labels for training. It's all about getting things ready for the big show.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "def prompt_mistral(dataset):\n",
    "    text =  f\"\"\"<s>[INST]{dataset['input']}[/INST] {dataset['output']}</s>\"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "max_length =400 \n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt_mistral(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### IV) Let's test the model before fine tunning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What total amount of funds was allocated for counter-narcotics support in the Defense Appropriations Act of 2024 (H.R. 4365)[/INST]?\n",
      "\n",
      "I apologize for any confusion, but I cannot directly provide you with the specific amount allocated for counter-narcotics support in the Defense Appropriations Act of 2024 (H.R. 4365) as I do not have access to that information in real time. You may want to check the text of the bill itself or contact the relevant congressional committees or the Department of Defense for the most accurate and up-to-date information on this matter. The bill text can be found on the official website of the Library of Congress at https://www.govinfo.gov/. Additionally, you may find it helpful to consult news articles or other reliable sources for more context and analysis on this issue.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logs except errors\n",
    "warnings.filterwarnings('ignore')  # Suppress Python warnings, including TensorFlow's\n",
    "\n",
    "eval_prompt = \"\"\"[INST] What total amount of funds was allocated for counter-narcotics support in the Defense Appropriations Act of 2024 (H.R. 4365)[/INST]?\n",
    "\"\"\"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### V) Let's set up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "<p>So, we're getting our model ready for some serious gym time to get it in shape. We're using this cool tool called <code>prepare_model_for_kbit_training</code> from PEFT to get it all prepped and set.</p>\n",
    "\n",
    "<p>Now, diving into the LoRA setup, think of <code>r</code> as how flexible our model's gonna be. It's like choosing between lifting light weights with more reps or heavy ones with fewer reps. More <code>r</code> means our model can learn more stuff, but it also means it's gonna work harder.</p>\n",
    "\n",
    "<p>Then, there's <code>alpha</code>, which is kinda like the protein shake for our weights. It decides how much oomph to give to the new moves our model's learning. Pumping up <code>alpha</code> is like saying, \"Hey, pay more attention to these new tricks!\"</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### VI) Training session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First up, we're importing the necessary stuff and checking if we've got more than one GPU to use. If we do, we're telling our model, \"Hey, you've got friends, let's work together!\" This makes things faster and smoother.</p>\n",
    "\n",
    "<p>We're also turning off some logging with <code>os.environ['WANDB_DISABLED'] = 'true'</code> we don't need to connect to the API on WANDB</p>\n",
    "\n",
    "<p>Next, we're setting up our training project and making sure our tokenizer knows what to use as a padding token, which is the end-of-sentence token here.</p>\n",
    "\n",
    "<p>Then, we're getting the <code>Trainer</code> ready with all its gear: the model, the datasets for training and validation, and a bunch of settings like how big our training batches should be, how often to save our progress, and when to check how well the model is doing. We're also telling it to use a specific optimizer that's really good at saving memory, which is great for big models.</p>\n",
    "\n",
    "<p>Last but not least, we're setting up a <code>DataCollator</code> that's going to help organize our data for language modeling, making sure everything's in tip-top shape for training. And with a final <code>trainer.train()</code>, we're off to the races!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "import os\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    \n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "project = \"uslaw-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"training_validation_loss.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### VII) Let's test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n",
    "\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-uslaw-finetune/checkpoint-750\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"[INST] What types of expenses are covered under the 'Other Procurement, Defense-Wide' category in H.R. 4365? [/INST]?\"\"\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
